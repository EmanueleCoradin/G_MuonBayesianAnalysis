---
title: "Binned Analysis"
author: "Emanuele Coradin and Dario Puggioni"
date: "`r Sys.Date()`"
output: 
  read_document: rmdformats::readthedown
  pdf_document:
    number_sections: true
    toc: true
    toc_depth: 2
  html_document:
    number_sections: true
    theme: spacelab
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

color_vector <- c("#CC0000",   # Dark red
                  "#CC79A7",   # Muted purple
                  "#D55E00",   # Vermilion
                  "#009E73",   # Bluish green
                  "#56B4E9",   # Sky blue
                  '#000046',   # Deep Blue
                  "#DB1E60",   # Pinkish-red
                  "#E69F00")   # Yellow-orange

```

```{r, message=FALSE, echo=FALSE}
library(rjags)
library(ggplot2)
library(dplyr)
```

# Bayesian analysis of the $\mu$ lifetime and g-2 parameter with $\mu$ SR

## Introduction

Muons are long-lived particles, produced in the decays of pions and
kaons originating from the interactions of primary cosmic rays with the
Earthâ€™s atmosphere.

Moreover, parity violation is also present in the decay which proceeds
as follow: $\mu^+ \rightarrow e^+ + \nu_e + \overline{\nu_\mu}$ and
$\mu^- \rightarrow e^- + \overline{\nu_e} + \nu_\mu$.

Simple experiments can be performed by measuring muons that decay in a
thick absorber. If the absorber is immersed in a constant magnetic
field, the muon spin, before the decay, precesses with its Larmor
frequency $\omega_L = g_{\mu} \frac{eB}{2 m_{\mu} c}$.

The decay proceeds mainly along the direction of the spin of the muon
and therefore, if the muon is (partly) polarized, the detected signal
varies with time with $\omega_L$. Analyzing the data collected without
and with the magnetic field, we set up a Markov Chain Monte Carlo that
allows us to extract the muon lifetime $\tau$, with the magnetic field
turned off, and the muon precession frequency with the magnetic field
turned on.

In this document we are presenting a conventional kind of analysis, by
building an histogram and then fitting it with a curve. The value of the
parameters are inferred in a Bayesian framework.

## Description of the data analysis

We have at our disposal two datasets: 'Lifetime' and 'Precession'. The
former contains the measurements of the time difference between the muon
(antimuon) and electron (positron) signals with the magnetic field
turned off, while in the latter the measurements are taken with a
magnetic field $B=5.6\ \text{mT}$. To convert the ADC counts in seconds
we use the results of a previous calibration measurement:
$t = p_0 + \text{ADC} \cdot p_1$, where
$p_0 = (7.4 \pm 4.4) \ \text{ns}$ and
$p_1 = (14.90 \pm 0.11) \ \text{ns}$ In particular, since the electron
could also undergo nuclear capture, the vast majority of the signals are
made of positrons.

Defining N(t) as the number of observed decays at time t + dt, we expect
it to follow the rule $N(t) = N_0 e^{-\frac t \tau} +c$ in the Lifetime
dataset, with c modeling a constant background.

If the magnetic field is turned on, we expect instead a different law
due to the precession of the antimuon:
$N(t) = N_0 e^{-\frac t \tau} (1+\alpha\cos(\omega_Lt+\delta) + c$ ,
where $\alpha$ is an asymmetry parameter originating from the parity
violation.

The analytic form of the differential emission probability per unit of
time of the positron is in fact: $$
d\Gamma = W(\varepsilon, \theta) \, d\varepsilon \, d\Omega = \frac{1}{4\pi \tau} \cdot 2\varepsilon^2 (3 - 2\varepsilon) \left[ 1 + \frac{2\varepsilon - 1}{3 - 2\varepsilon} \cos \theta \right] d\varepsilon \, d\Omega = \frac{1}{4\pi \tau} \cdot K(\varepsilon)[1+a(\varepsilon) \cos(\theta)] d\varepsilon \, d\Omega
$$ where $\epsilon = \frac{\text{E}}{\text{E}_{\text{max}}}$,
$K(\varepsilon) = 2\varepsilon^2(3-2\varepsilon)$,
$a(\varepsilon) = \frac{2\varepsilon-1}{3-2\varepsilon}$,
$\text{E}_{\text{max}} \approx 53.83\ \text{MeV}$, $\theta=0$ is the
spin direction.

The asymmetry term is thus
$\alpha(\varepsilon) = K(\varepsilon)a(\varepsilon)$, and it's energy
dependent with support $\alpha\in \left[ -\frac 1 3,\ 1 \right]$. We can
calculate its mean value and variance: 

-   $\bar{\alpha} = \int_0^1 \alpha(\varepsilon) \, d\varepsilon= \frac{1}{3}$ 
-   $\text{Var}[\alpha] = \int_0^1 (\alpha(\varepsilon)-\bar{\alpha})^2 \, d\varepsilon \approx 0.3$

This derivation is valid in the hypothesis of maximum and equal
detection efficiency for every energy in the spectrum. In practice,
however, its mean value is considerably lower.

Furthermore, the angular distribution of cosmic muons is experimentally
close to $P(\theta) = I_0 \cos^2(\theta)$, where $\theta$ is the
azimuthal angle.

Considering a detector characterized by a cylindrical symmetry along the
azimuth direction, and immersed in a uniform transverse magnetic field
$B$, we can extrapolate that the pdf of observing a decay at time $t$ is
$$
P(t) = \frac{e^{-\frac{t}{\tau}} (1+\alpha \cos(\omega_L t + \delta))}{Z}
$$ with $\alpha$ accounting for the experimental asymmetry, and $\delta$
modeling errors in the definition of the time zero or angular
misalignments.

## Choice of the priors

### Muon Lifetime $\tau$

For the muon lifetime $\tau$, we assume a prior that reflects our prior
knowledge based on similar previous experiments:

-   Amsler: $\tau = (2.10\pm0.05)\ \mu\text{s}$;
-   Bosnam: $\tau = (2.16\pm0.02)\ \mu\text{s}$ and
    $\tau = (2.07\pm0.02)\ \mu\text{s}$

Considering a weighted average of them, we obtain a prior mean of
$\bar{\tau_{\text{prior}}} = 2.11\  \mu\text{s}$ with standard deviation
of $\text{SD}[{\tau_{\text{prior}}}] = 0.01  \mu\text{s}$

Therefore, we decide to model the prior with a gamma distribution of
parameters $\alpha$ and $\beta$ tuned to satisfy these conditions.

### Asimmetry parameter $\alpha$

The most conservative choice would be a prior fully based on the
theoretical aspects mentioned above. Nevertheless, since the
experimental setup is similar to the one used in the previous
experiments we previously cited, we decide again to use them:

-   Amsler: $\alpha = 0.067 \pm 0.011$;
-   Bosnam: $\alpha = 0.05  \pm 0.01$ and $\alpha = 0.06  \pm 0.01$.

This time we are more conservative, choosing a prior mean of 0.06 with a
standard deviation of 0.01. We model the pdf using a re scaled and
shifted Beta distribution, having support in
$\alpha\in \left[-\frac 1 3,\ 1 \right]$.

### Larmor frequency $\omega_L$

Recall that the Larmor frequency formula is
$\omega_L = g_{\mu} \frac {eB} {2 m_{\mu} c}$. Considering $g_{\mu}=2$,
the magnetic field $B=5.6\ \text{mT} \pm 2\%$, and all the other known
constants, we can calculate the expected value to be
$\omega_L = 4.77\ \text{MHz}$.

The main source of error in this estimate is by far the uncertainty in
the magnetic field. Assuming a uniform distribution, we can convert the
maximum error in a casual one $\sigma_B = \frac {\Delta B} {\sqrt{12}}$.
Projecting this back to the Larmor frequency, we can estimate its casual
error to be $\sigma_{\omega_L} = 0.06\ \text{mT}$.

Therefore, we decide to model the prior with a gamma distribution of
parameters $\alpha$ and $\beta$ tuned to satisfy these conditions.

### Initial phase $\delta$

Given the theoretical considerations made above, considering the
experimental evidence that the probability of observing a cosmic muon at
the azimuthal angle $\theta$ is $P(\theta) = I_0 \cos^2(\theta)$, we
expect the mean value to be 0 with a variance
$\text{Var}[\theta]= \int_{-\frac \pi 2}^{\frac \pi 2} I_0 \theta^2 \cos^2(\theta)\ d\theta = 0.32$.

Again, we model the prior distribution with a rescaled and shifted beta
distribution, in order to have support in
$\theta \in \left [-\frac \pi 2, \frac \pi 2 \right]$ and those moments.

### $N_0$ and c

These parameters are modeling the number of true observed decays and the
background rate. Since they depend both on the time of measurement and
the behaviour of the all detector they are really difficult to estimate,
so we decide to take just a uniform prior.

## Functions and default values

```{r functions}

# ------- Analytical calculations -----------

getGammaParam <- function(mean, variance) {
  beta <- mean / variance
  alpha <- mean * beta
  c(alpha = alpha, beta = beta)
}

getBetaParam <- function(mean, variance) {
  nu <- mean * (mean * (1 - mean) / variance - 1)
  alpha <- mean * nu
  beta <- nu * (1 - mean)
  c(alpha = alpha, beta = beta)
}

# -------- Functions for plotting -----------

plotdata <- function(data, title = NULL, xlab = expression(Delta * T ~ "[" * mu * "s" * "]"), histdata = NULL, scale = 'lin', bins = bins_default, plot_histbar = FALSE, xlim = NULL, ylim=NULL) {
  if (is.null(histdata)) histdata <- hist(data, breaks = bins, plot = FALSE)

  df <- data.frame(mids = histdata$mids, counts = histdata$counts)

  if (scale == "lin") {
    df <- df %>% mutate(se = sqrt(counts))  # Errore standard come sqrt dei counts (assunzione di distribuzione di Poisson)
  } else {
    df <- df %>% mutate(se = 1 / (log(10)*sqrt(counts)))  # Errore standard come 1/sqrt(counts)
  }

  p <- ggplot() +
    theme_minimal() +
    ggtitle(title) +
    labs(x = xlab, y = ifelse(scale == "lin", "Counts", expression(log[10](Counts))), color = "Legend") +
    theme(plot.title = element_text(face = "bold", size = 14),
          axis.title.x = element_text(size = 12),
          axis.title.y = element_text(size = 12))

  if (plot_histbar) {
    p <- p + geom_bar(data = df, aes(x = mids, y = counts), fill = 'blue', color = 'blue')
    if (scale == "log") {
      p <- p + scale_y_log10()
    }
  } else {
    if (scale == "lin") {
      p <- p +
        geom_point(data = df, aes(x = mids, y = counts), color = 'blue', size = 1) +
        geom_errorbar(data = df, aes(x = mids, ymin = counts - se, ymax = counts + se), color = 'red', width = 0.2, size = 0.5, alpha = 0.6)
    } else {
      p <- p +
        geom_point(data = df, aes(x = mids, y = log10(counts)), color = 'blue', size = 1) +
        geom_errorbar(data = df, aes(x = mids, ymin = log10(counts) - se, ymax = log10(counts) + se), color = 'red', width = 0.2, size = 0.5, alpha = 0.6)
    }
  }

  if (!is.null(xlim)) {
    p <- p + xlim(xlim)
  }
  if (!is.null(ylim)) {
    p <- p + ylim(ylim)
  }

  p
}

fitting <- function(law, parms, data, xlab = expression(Delta * T ~ "[" * mu * "s" * "]"), scale = 'lin', ...) {
  p <- plotdata(data, xlab = xlab, scale = scale, ...)
  if (scale == 'lin') {
    p <- p + geom_line(aes(x = data, y = law(data, parms)), size = 0.8, show.legend = FALSE)
  } else if (scale == 'log') {
    p <- p + geom_line(aes(x = data, y = log10(law(data, parms))), size = 0.8, show.legend = FALSE)
  }
  p
}

plot_posterior_param <- function(samples, param_name, stats, prior_func=NA, xlim=NULL, xlab=NULL,  legend.loc="topright", ...) {
  dens <- density(samples)
  
  if(is.null(xlab))
    xlab <- param_name

  # Calculate the prior density if prior_func is provided
  if(!is.na(prior_func)){
    if(is.null(xlim)) {
      x <- seq(min(samples), max(samples), length = 5000)
    } else {
      x <- seq(xlim[1], xlim[2], length = 5000)
    }
    y_prior <- prior_func(x)
    max_prior <- max(y_prior)
  } else {
    max_prior <- 0
  }

  # Calculate the maximum y value for posterior and prior, then set ylim
  max_posterior <- max(dens$y)
  max_y <- max(max_posterior, max_prior) * 1.1

  plot(dens, main = paste("Posterior Density of", param_name), xlab = xlab, ylab = "Probability Density", col = "skyblue", lwd = 2, xlim=xlim, ylim=c(0, max_y), ...)

  # Overlay the prior density if prior_func is provided
  if(!is.na(prior_func)){
    lines(x, y_prior, col = "darkorange", lwd = 2, lty = 2)
    
    legend(legend.loc, legend = c(
      paste("Mean =", round(stats$mean, 2)),
      paste("SD =", round(stats$sd, 2)),
      paste("2.5% CI =", round(stats$ci[1], 2)),
      paste("97.5% CI =", round(stats$ci[2], 2)),
      "Prior"
      ), 
      col = c("blue", 'lightblue', "red", "red", "darkorange"), lty = c(2, 2, 2, 2), lwd = 2, bty = "n"
    )
  } else {
    legend(legend.loc, legend = c(
      paste("Mean =", round(stats$mean, 2)),
      paste("SD =", round(stats$sd, 2)),
      paste("2.5% CI =", round(stats$ci[1], 2)),
      paste("97.5% CI =", round(stats$ci[2], 2))
      ), 
      col = c("blue", 'lightblue', "red", "red"), lty = c(2, 2, 2), lwd = 2, bty = "n"
    )
  }
  
  abline(v = stats$mean, col = "blue", lwd = 2, lty = 2)
  abline(v = stats$mean + stats$sd, col = "lightblue", lwd = 2, lty = 2)
  abline(v = stats$mean - stats$sd, col = "lightblue", lwd = 2, lty = 2)
  abline(v = stats$ci, col = "red", lwd = 2, lty = 2)
}


extract_stats <- function(summary_obj, param_name) {
  list(
    mean = summary_obj$statistics[param_name, "Mean"],
    sd = summary_obj$statistics[param_name, "SD"],
    ci = summary_obj$quantiles[param_name, c("2.5%", "97.5%")]
  )
}

```

```{r Default values}
bins_default <- 120
bins_lifetime <- 60

# Named vector containing the moments for the priors
prior_moments <- c( #like a dictionary
  tau_mean = 2.1,    tau_var = 0.01^2,
  alpha_mean = 0.06,  alpha_var = 0.01^2,
  omega_mean = 4.77, omega_var = (4.77 * 0.04)^2 / 12,
  delta_mean = 0,    delta_var = 0.32
)

# Named vector containing the parameters of the priors
PriorParam <- list(
  tau = getGammaParam(prior_moments[['tau_mean']], prior_moments[['tau_var']]),        #[[]] selects just the value, not the name
  omega = getGammaParam(prior_moments[['omega_mean']], prior_moments[['omega_var']]),
  delta = getBetaParam((prior_moments[['delta_mean']] + pi/2) / pi, prior_moments[['delta_var']] * pi^-2), #beta is in interval [0,1], so we dilatate and shift the parameters, then we'll reset the function
  alpha = getBetaParam((prior_moments[['alpha_mean']] + 1/3) / (4/3), prior_moments[['alpha_var']] / (4/3)^2)
)

```

## Data Loading and Filtering

```{r }
# Read files
paths_lifetime   <-dir('g-2data/data/lifetime/2023_24', pattern ='^t', full.names=TRUE)
paths_precession <-dir('g-2data/data/precession/2023_24', full.names=TRUE)

data_lifetime   <-numeric()
data_precession <- numeric()

for (path in paths_lifetime)   {data_lifetime   <- as.integer(c(data_lifetime, (readLines(path))))}
for (path in paths_precession) {data_precession <- as.integer(c(data_precession, (readLines(path))))}

cat('Length data_lifetime=',length(data_lifetime),'\n')
cat('Length data_precession',length(data_precession))

# Calibrate data
p0       <- 7.4
sigma_p0 <- 4.4
p1       <- 14.90
sigma_p1 <- 0.11

calibrate <- function(x) (p0 + p1 * x)*1e-3

data_lifetime   <- calibrate(data_lifetime)
data_precession <- calibrate(data_precession)
```

```{r }
plotdata(data_lifetime, title = 'Lifetime Dataset', scale = 'log', plot_histbar = FALSE, bins = bins_lifetime) #data, xlab, histdata = NULL, scale, bins, plot_histbar, xlim
plotdata(data_precession, title = 'Precession Dataset', scale=  'log', plot_histbar = FALSE)
```

```{r }
lifetime_hist<-hist(data_lifetime, breaks=bins_lifetime, plot=FALSE) # from each dataset we create a df from a hist
filtered_lifetime <- data.frame(mids = lifetime_hist$mids, counts = lifetime_hist$counts)
filtered_lifetime <- filtered_lifetime[-1, ]

precession_hist<-hist(data_precession, breaks=bins_default, plot=FALSE)
filtered_precession <- data.frame(mids = precession_hist$mids, counts = precession_hist$counts)
filtered_precession <- filtered_precession[-1, ]

plotdata(data_lifetime, title = 'Lifetime', histdata = filtered_lifetime, scale = 'log', plot_histbar = FALSE)
plotdata(data_precession, title = 'Precession', histdata = filtered_precession, scale=  'log', plot_histbar = FALSE)
```

## Lifetime analysis

```{r }
x <- filtered_lifetime$mids
y <- filtered_lifetime$counts

# Define initial values for the chains
init_values <- list(list(N = 10000, c = 5, tau = 2.))

n_burnin <- 5000  # Length of the burn-in phase
thinning <- 2
Nrep     <- 1e5 # Number of values to simulate
n_adapt  <- 2000


# Define the model string using sprintf
model_lifetime_def <- sprintf("
model{
  # Likelihood
  for (i in 1:length(x)) {
    I[i] <- N*exp(-x[i]/tau) + c
    y[i] ~ dpois(I[i])
  }

  # Prior
  N ~ dunif(0, 20000)
  c ~ dunif(0, 50)
  tau ~ dgamma(%.2f, %.2f)
}", PriorParam$tau[["alpha"]], PriorParam$tau[["beta"]])

dataList = list(x = x, y = y)

# Create the model
model_lifetime <- jags.model(file = textConnection(model_lifetime_def), data = dataList, inits = init_values, n.chains = 1)

# Adaptation phase
adapt(model_lifetime, n_adapt)

# Burn-in phase
update(model_lifetime, n.iter = n_burnin)

# Sample from the posterior
posterior_lifetime <- coda.samples(model_lifetime, variable.names = c('N','c','tau'), n.iter = Nrep, thin = thinning)
(summary_lifetime <- summary(posterior_lifetime))
plot(posterior_lifetime)

posterior_matrix <- as.matrix(posterior_lifetime)

# Retrieve the chains
N_samples   <- posterior_matrix[, "N"]
c_samples   <- posterior_matrix[, "c"]
tau_samples <- posterior_matrix[, "tau"]

acf(N_samples,  main = "Autocorrelation of N")
acf(c_samples,   main = "Autocorrelation of c")
acf(tau_samples, main = "Autocorrelation of tau")
```

```{r }
N_stats <- extract_stats(summary_lifetime, "N")
c_stats <- extract_stats(summary_lifetime, "c")
tau_stats <- extract_stats(summary_lifetime, "tau")

prior_tau <- function(x) {
  dgamma(x, shape = PriorParam$tau[["alpha"]], rate = PriorParam$tau[["beta"]])
}

plot_posterior_param(N_samples, "N", N_stats)
plot_posterior_param(c_samples, "c", c_stats)
plot_posterior_param(tau_samples, "tau", tau_stats, prior_tau, xlim=c(2.03,2.23), xlab=expression(tau ~ "[" * mu * "s" * "]"), legend.loc="topleft")

```

```{r }
attributes(summary_lifetime)
parms_lt <- summary_lifetime$statistics[,'Mean']

lifetime_law <- function(x, parms) parms['N'] * exp(-x/parms['tau']) + parms['c']

fitting(lifetime_law,parms_lt,data_lifetime, histdata = filtered_lifetime, scale="lin", title='Fit of the Lifetime dataset')
fitting(lifetime_law,parms_lt,data_lifetime, histdata = filtered_lifetime, scale="log", title='Fit of the Lifetime dataset')
```

## Precession Analysis

```{r }
tau_lifetime_posterior <- getGammaParam(summary_lifetime$statistics['tau', 'Mean'] ,summary_lifetime$statistics['tau', 'SD'])
```

```{r }
x <- filtered_precession$mids
y <- filtered_precession$counts

n_burnin <- 5000  # Length of the burn-in phase
thinning <- 2
Nrep     <- 1e5 # Number of values to simulate
n_adapt  <- 1000

# Define initial values for the chains
#init_values <- list(
#  list(N = 500, c = 10, delta_base = 0.4, omega = 4.7, alpha_base = 0.05, tau=2.2)
#)

init_values <- list(
  list(N = 500, c = 10, delta_base = 0.4, omega = 4.7, alpha_base = 0.05)
)

# Define the model string using sprintf
model_precession_def <- sprintf("
model{
  # Likelihood
  for (i in 1:length(x)) {
    I[i] <- N*exp(-x[i]/tau) * (1 + alpha*cos(omega*x[i] + delta)) + c
    y[i] ~ dpois(I[i])
  }

  # Prior
  N ~ dunif(0, 15000)
  c ~ dunif(0, 100)

  delta_base ~ dbeta(%.2f, %.2f)
  omega ~ dgamma(%.2f, %.2f)
  alpha_base ~ dbeta(%.2f, %.2f)
  #tau ~ dgamma(%.2f, %.2f)

  delta <- delta_base * 3.14 - 3.14/2
  alpha <- alpha_base * 4/3 - 1/3
  
  g <- 0.41986*omega
}
", PriorParam$delta["alpha"], PriorParam$delta["beta"],
   PriorParam$omega["alpha"], PriorParam$omega["beta"],
   PriorParam$alpha["alpha"], PriorParam$alpha["beta"],
   tau_lifetime_posterior["alpha"], tau_lifetime_posterior["beta"])

dataList = list(x = x, y = y, tau=summary_lifetime$statistics['tau', 'Mean'])

# Create the model
model_precession <- jags.model(file = textConnection(model_precession_def), data = dataList, inits = init_values, n.chains = 1)

# Adaptation phase
adapt(model_precession, n_adapt)

# Burn-in phase
update(model_precession, n.iter = n_burnin)

# Sample from the posterior
posterior_precession <- coda.samples(model_precession, variable.names = c('N', 'c', 'delta', 'omega', 'alpha', 'g', 'tau'), n.iter = Nrep, thin = thinning)
(summary_precession <- summary(posterior_precession))

posterior_matrix <- as.matrix(posterior_precession)

# Retrieve the chains
N_samples    <- posterior_matrix[, "N"]
c_samples    <- posterior_matrix[, "c"]
delta_samples <- posterior_matrix[, "delta"]
omega_samples <- posterior_matrix[, "omega"]
alpha_samples <- posterior_matrix[, "alpha"]
g_samples <- posterior_matrix[, "g"]


acf(N_samples, main = "Autocorrelation of N")
acf(c_samples, main = "Autocorrelation of c")
acf(delta_samples, main = "Autocorrelation of delta")
acf(omega_samples, main = "Autocorrelation of omega")
acf(alpha_samples, main = "Autocorrelation of alpha")
acf(g_samples, main = "Autocorrelation of g")
```

```{r }
# Extract statistics for each parameter
N_stats <- extract_stats(summary_precession, "N")
c_stats <- extract_stats(summary_precession, "c")
delta_stats <- extract_stats(summary_precession, "delta")
omega_stats <- extract_stats(summary_precession, "omega")
alpha_stats <- extract_stats(summary_precession, "alpha")
g_stats <- extract_stats(summary_precession, "g")

# Define the prior functions for new parameters
prior_delta <- function(x) {
  dbeta((x + 3.14/2) / 3.14, shape1 = PriorParam$delta["alpha"], shape2 = PriorParam$delta["beta"]) / 3.14
}

prior_omega <- function(x) {
  dgamma(x, shape = PriorParam$omega["alpha"], rate = PriorParam$omega["beta"])
}

prior_alpha <- function(x) {
  dbeta((x + 1/3) * 3 / 4, shape1 = PriorParam$alpha["alpha"], shape2 = PriorParam$alpha["beta"]) * 3 / 4
}

# Plotting posterior parameters with priors
#par(mfrow = c(5, 1))  # Set up the plotting area for 5 plots vertically

plot_posterior_param(N_samples, "N", N_stats)
plot_posterior_param(c_samples, "c", c_stats)
plot_posterior_param(delta_samples, "delta", delta_stats, prior_delta, xlab = expression(delta ~ '[rad]'))
plot_posterior_param(omega_samples, "omega", omega_stats, prior_omega, xlab = expression(omega ~ '[MHz]' ))
plot_posterior_param(alpha_samples, "alpha", alpha_stats, prior_alpha, xlab = expression(alpha), xlim = c(-0.02, .12))
plot_posterior_param(g_samples, "g", g_stats, xlab=expression('g' * mu))
```

```{r }
precession_law <- function(x, parms) {
  parms['N'] * exp(-x / parms['tau']) * (1 + parms['alpha'] * cos(parms['omega'] * x + parms['delta'])) + parms['c']
}

parms_precession <- summary(posterior_precession)$statistics[,'Mean']

fitting(precession_law, parms_precession, data_precession, title='Precession', scale='lin', histdata=filtered_precession)
fitting(precession_law, parms_precession, data_precession, title='Precession', scale='log', histdata=filtered_precession)
```
